---
title: "Hyperparameter Optimization for Large Language Model Instruction-Tuning"
collection: publications
category: manuscripts
permalink: /publication/2023-12-01-AAAI
excerpt: 'The fine-tuning of Large Language Models (LLMs) has enabled them to recently achieve milestones in natural language processing applications. The emergence of ever larger LLMs has paved the way for more efficient fine-tuning methods. Among these, the Low-Rank Adaptation (LoRA) method keeps most of the weights of the pre-trained LLM frozen while introducing a low-rank decomposition of the weight matrix, enabling the tuning of only a very small proportion of the network. The performance on downstream tasks of models fine-tuned with LoRA heavily relies on a set of hyperparameters including the rank of the decomposition. In this work, we investigate the choice of these hyperparameters through two main blackbox optimization (BBO) techniques. We examine the whole pipeline of performing fine-tuning and validation on a pre-trained LLM as a blackbox and efficiently explore the space of hyperparameters with the \nomad algorithm, achieving a boost in performance and human alignment of the tuned model. '
date: 2023-12-01
venue: 'Association for the Advancement of Artificial Intelligence (AAAI) - Edge Intelligence Workshop'
paperurl: 'https://arxiv.org/pdf/2312.00949'
citation: 'Tribes, C., Benarroch-Lelong, S., Lu, P., & Kobyzev, I. (2023). <i>Hyperparameter optimization for large language model instruction-tuning</i>. AAAI24: Edge Intelligence Workshop.'
---

The contents above will be part of a list of publications, if the user clicks the link for the publication than the contents of section will be rendered as a full page, allowing you to provide more information about the paper for the reader. When publications are displayed as a single page, the contents of the above "citation" field will automatically be included below this section in a smaller font.
